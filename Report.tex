\documentclass[12pt, titlepage, a4paper]{article}

\title{Perceptrons: An Overview}
\author{Harjas Monga and Vidhur Kumar}

\begin{document}
	\maketitle

	\textit{"We expect the perceptron to be the embryo of an electronic computer that will be able to walk, talk, see, write, reproduce itself and be conscious of its existence."} - The New York Times, 1958.	
	
	\section{Introduction: What is a Perceptron?}
	
	The perceptron is an algorithm for learning a binary classifier: a function that maps its input $x$ (a real-valued \textbf{vector}) to an output value $f(x)$ (a single binary value). Shown below is a mathematical definition: \\
	
	\begin{center}
		$f(x) = $
		$
\begin{array}{cc}
  \{ & 
    \begin{array}{cc}
    	1 & wx + b > 0 \\
    	0 & otherwise
    \end{array}
\end{array}
		$
	\end{center}
	
	\section{Workings: What drives a Perceptron?}
		
		\subsection{Mathmatical Model}	
		
		A perceptron is nothing more than a function that takes in several inputs and produces an output of either 0 or 1. \\ \\
		The input of perceptron is usually modeled as a column vector. Let $A(n\times1)$ represent this column vector. The perceptron applies weights to each values in the $A$ and then sums them together. In other words, the perceptron finds a linear combination of the 
	\section{Applications: Why are Perceptrons useful?}
	
	
	\section{Evolution: What has the idea of a Perceptron led to?}
\end{document}