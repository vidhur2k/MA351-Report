
\subsection{Mathmatical Model}	
		
A perceptron is nothing more than a function that takes in several inputs and produces an output of either 0 or 1.\\ \\
The input of perceptron is usually modeled as a column vector. Let $A(n\times1)$ represent this column vector. The perceptron applies weights to each values in the $A$ and then sums them together. In other words, the perceptron finds a linear combination of the matrix $A$ as show below. At first these weights are random, however, while training the weights get set to the optimal values through a process called back propagation. The inner works of back propagation will be covered later.
$$
A = 
\begin{bmatrix}
	a_1 \\
	a_2 \\
	a_3 \\
	\vdots \\
	a_n
\end{bmatrix} 
$$
