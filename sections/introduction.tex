
\subsection{Mathmatical Model}	
		
A perceptron is nothing more than a function that takes in several inputs and produces an output of either 0 or 1.\\ \\
The input of perceptron is usually modeled as a column vector. Let $X(n\times1)$ represent this column vector. The perceptron applies weights to each values in the $X$ and then sums them together. In other words, the perceptron finds a linear combination of the matrix $X$ as show below. At first these weights are random, however, while training the weights get set to the optimal values through a process called back propagation. The inner works of back propagation will be covered later.
$$
X = 
a change 
\begin{bmatrix}
	x_1 \\
	x_2 \\
	x_3 \\
	\vdots \\
	x_n
\end{bmatrix} 
$$
